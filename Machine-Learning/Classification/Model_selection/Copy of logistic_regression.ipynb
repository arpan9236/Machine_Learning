{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"15fKa6MRzX09W2OlBKiPbKqRIg-S1AIZb","timestamp":1720606472274}],"machine_shape":"hm","toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["One of the very important question asked to the data science community is which classification model should I select? In this section we will learn how to select quickly and efficiently the **best classification model** for the given dataset having any number of features and any number of observations.\n","\n","In this **Model_selection** folder we have all the classification models that we learnt or implemented in this Part-3. In this folder, in all of the codes, we have removed the print commands to simplify the implementation. And also we removed the visualization part on the training and test sets because the visualization only works when we have only two features in the dataset.\n","\n","**About the dataset:** \\\\\n","Here we take the calssic dataset of classification of breast cancer: bening tumor or malignant tumor, from UCI ML repository. This dataset is a generic one containing many features, all are numerical features, and a binary dependent variable vector (the 'class' column) taking values 2 (bening tumor) or 4 (malignant tumor). Each row corresponds to a patient and for each of this patient we gathered 11 information (10 features: all the left columns and 1 dependent variable: the most right column). Doctors understand the meaning of each feature, but data scientist do not know about the meaning of each atttribute/feature. But this is fine as the data scientist can still build classification ML models and understand the correlations between all the features and dependent variable vector. Finally the classification model trained on the training set will be able to predict tumor type (benign:2 or malignant:4) for each patient (new or old).\n","\n","The basic format of all the code templates in this folder is: (i) Data pre-processing, (ii) Training the build ML classification model on the training set and (iii) Evaluation of the trained model on the test set by computing the confusion matrix and accuracy score.\n","\n","The only thing change from one code template to another is only one cell in which we build and train the ML model we wanna try.\n","\n","The code templates in this folder is valid as long as, the dataset contains the features in the left columns and the dependent variable vector in the right most column the below format is going to work, regardless of the number of features in the dataset, and for the numerical features only. If there is categorical feature, don't forget to use data preprocessing tools (like one hot encoding or ordinal encoding).\n","\n","This code templates are made just modify very minimal like the name of the dataset."],"metadata":{"id":"YAxNJ3ENJCp2"}},{"cell_type":"markdown","metadata":{"id":"0MRC0e0KhQ0S"},"source":["# Logistic Regression"]},{"cell_type":"markdown","metadata":{"id":"LWd1UlMnhT2s"},"source":["## Importing the libraries"]},{"cell_type":"code","metadata":{"id":"YvGPUQaHhXfL"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K1VMqkGvhc3-"},"source":["## Importing the dataset"]},{"cell_type":"code","metadata":{"id":"M52QDmyzhh9s"},"source":["# As long as, the dataset contains the features in the left columns and the dependent variable vector in the\n","# right most column the below format is going to work, regardless of the number of features in the dataset.\n","\n","dataset = pd.read_csv('Breast_cancer_Wisconsin.csv')\n","X = dataset.iloc[:, :-1].values   # Values of all the rows and all the columns, except the last columns is put into X, the matrix of features.\n","y = dataset.iloc[:, -1].values   # Values of all the rows and only the last column is put into y, the dependent variable vector."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YvxIPVyMhmKp"},"source":["## Splitting the dataset into the Training set and Test set"]},{"cell_type":"code","metadata":{"id":"AVzJWAXIhxoC"},"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kW3c7UYih0hT"},"source":["## Feature Scaling"]},{"cell_type":"code","metadata":{"id":"9fQlDPKCh8sc"},"source":["from sklearn.preprocessing import StandardScaler\n","sc = StandardScaler()\n","X_train = sc.fit_transform(X_train)\n","X_test = sc.transform(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bb6jCOCQiAmP"},"source":["## Training the Logistic Regression model on the Training set"]},{"cell_type":"code","metadata":{"id":"e0pFVAmciHQs","colab":{"base_uri":"https://localhost:8080/","height":75},"executionInfo":{"status":"ok","timestamp":1720608859142,"user_tz":-330,"elapsed":17,"user":{"displayName":"ARPAN DAS","userId":"14423431301870891264"}},"outputId":"bfcd5f6a-f0aa-416e-f4dd-5d12396ddc4e"},"source":["from sklearn.linear_model import LogisticRegression\n","classifier = LogisticRegression(random_state = 0)\n","classifier.fit(X_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(random_state=0)"],"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(random_state=0)</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"h4Hwj34ziWQW"},"source":["## Making the Confusion Matrix"]},{"cell_type":"code","metadata":{"id":"D6bpZwUiiXic","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1720608859143,"user_tz":-330,"elapsed":13,"user":{"displayName":"ARPAN DAS","userId":"14423431301870891264"}},"outputId":"3b448eb9-6493-4be7-c377-f274c0f03382"},"source":["from sklearn.metrics import confusion_matrix, accuracy_score\n","y_pred = classifier.predict(X_test)\n","cm = confusion_matrix(y_test, y_pred)\n","print(cm)\n","accuracy_score(y_test, y_pred)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[103   4]\n"," [  5  59]]\n"]},{"output_type":"execute_result","data":{"text/plain":["0.9473684210526315"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["So, in logistic regression, we got only 4+5=9 incorrect predictions and accuracy = 94.7%. Great!.\n","\n","Below we tabulate the accuracy score for each classification model used in this dataset.\n","\n","**Classification model**    $~~~~~~~~~~~~~$       **Accuracy score**  \\\\\n","Logistic regression     $~~~~~~~~~~~~~~~~~~~~~~$            94.7%  \\\\\n","K-nearest neighbor (K-NN)     $~~~~~~~~~~~~$            94.7%  \\\\\n","Support vector machine (SVM)     $~~~~~$            94.15%  \\\\\n","Kernel SVM     $~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$            95.3%  \\\\\n","Naive Bayes    $~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$            94.15%  \\\\\n","Decision tree classification     $~~~~~~~~~~~$            95.9%  \\\\\n","Random forest classification     $~~~~~~~~~$            93.6%  \\\\\n","\n","So, we see that the big winner in this dataset is Decision tree classification. Usually Random forest classification become winner for complex dataset having many features and many observations, but in this case that is not the case. Very surprising result!! That is why we need to apply all the ML models and chose the best fit one.\n"],"metadata":{"id":"BODOIiRnSf5y"}}]}